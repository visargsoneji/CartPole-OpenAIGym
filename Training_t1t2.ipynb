{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notebook by Visarg,Avil and Aryan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training code for task1 and task2(trained on task1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The output will be Scores with the corresponding episode number and the training log(graph of Episodes v/s Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --versions--\n",
    "# numpy==1.18.2\n",
    "# gym==0.17.1\n",
    "# Keras==2.3.1\n",
    "# Keras-Applications==1.0.8\n",
    "# Keras-Preprocessing==1.1.0\n",
    "# tensorboard==2.1.1\n",
    "# tensorflow==2.1.0\n",
    "# tensorflow-estimator==2.1.0\n",
    "# matplotlib==3.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import gym \n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam  \n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(): \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.state_size = self.env.observation_space.shape[0] \n",
    "        self.action_size = self.env.action_space.n \n",
    "        \n",
    "        self.gamma = 0.95\n",
    "        self.learning_rate = 0.001\n",
    "        self.batch_size = 20\n",
    "        \n",
    "        self.epsilon = 1 \n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "        \n",
    "        self.memory = deque(maxlen = 1200) \n",
    "        \n",
    "        self.model = self.build_model()\n",
    "    \n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(48, input_dim = self.state_size, activation = 'tanh')) \n",
    "        model.add(Dense(24,activation = 'tanh')) \n",
    "        model.add(Dense(self.action_size, activation = 'linear')) \n",
    "        model.compile(loss = 'mse', optimizer = Adam(lr = self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        if random.uniform(0,1) <= self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            act_values = self.model.predict(state)\n",
    "            return np.argmax(act_values[0])\n",
    "              \n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return \n",
    "        \n",
    "        minibatch = random.sample(self.memory, self.batch_size) \n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            if done: \n",
    "                target = reward\n",
    "            else:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0]) \n",
    "                \n",
    "            train_target = self.model.predict(state)\n",
    "            train_target[0][action] = target\n",
    "            self.model.fit(state, train_target, verbose = 0) \n",
    "    \n",
    "    def exploration(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def train(self):\n",
    "        episodes=[]\n",
    "        final_scores=[]\n",
    "        for e in range(500): # maximum episodes to train = 500\n",
    "            \n",
    "            state = self.env.reset() #reseting env for each episode\n",
    "            state = np.reshape(state, [1,4])\n",
    "            \n",
    "            score = 0 #initialising score for each episode\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action = self.act(state) #choosing action according to the state\n",
    "                next_state, reward, done, _ = self.env.step(action) #taking action from above step\n",
    "                next_state = np.reshape(next_state, [1,4]) \n",
    "                self.remember(state, action, reward, next_state, done) #pushing info in deque\n",
    "                state = next_state #updating state\n",
    "                self.exploration() #updating epsilon\n",
    "                score+=1 #incrementing score as the reward for survival is 1\n",
    "                \n",
    "                if done: #when game(episode) is over\n",
    "                    final_scores.append(score)\n",
    "                    episodes.append(e+1)\n",
    "                    print(\"Episode: {}, Score: {}\".format(e+1, score))\n",
    "                    if score == 500: # Stoping the training when score reaches 500 for the first time\n",
    "                        print(\"Number of episodes to train : {}\".format(e+1))\n",
    "                        print(\"--Saving trained model--\")\n",
    "                        self.model.save(\"model_t1t2.h5\") #saving model\n",
    "                        #Training logs\n",
    "                        plt.figure(1) \n",
    "                        plt.plot(episodes,final_scores)\n",
    "                        plt.xlabel('Episodes')\n",
    "                        plt.ylabel('Scores')\n",
    "                        plt.title('Training')\n",
    "                        plt.show()\n",
    "                        return\n",
    "                self.replay() #updating weights of NN for better prediction\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent = Agent()\n",
    "agent.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
