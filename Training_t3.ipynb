{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notebook by Visarg,Avil and Aryan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training code for task3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The output will be Scores with the corresponding episode number and the training log(graph of Episodes v/s Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --versions--\n",
    "# numpy==1.18.2\n",
    "# gym==0.17.1\n",
    "# Keras==2.3.1\n",
    "# Keras-Applications==1.0.8\n",
    "# Keras-Preprocessing==1.1.0\n",
    "# tensorboard==2.1.1\n",
    "# tensorflow==2.1.0\n",
    "# tensorflow-estimator==2.1.0\n",
    "# matplotlib==3.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import gym \n",
    "from collections import deque\n",
    "from keras.models import Model\n",
    "from keras.layers import Input,Dense\n",
    "from keras.optimizers import RMSprop \n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(): \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.state_size = self.env.observation_space.shape[0] \n",
    "        self.action_size = self.env.action_space.n \n",
    "    \n",
    "        self.gamma = 0.95\n",
    "        self.learning_rate = 0.00025\n",
    "        self.batch_size = 40\n",
    "    \n",
    "        self.epsilon = 1 \n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.001\n",
    "        \n",
    "        self.memory = deque(maxlen = 2000)\n",
    "        \n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        input_shape=(self.state_size,)\n",
    "        action_space=self.action_size\n",
    "        X_in = Input(input_shape)\n",
    "        X = Dense(512, input_shape=input_shape, activation=\"relu\", kernel_initializer='he_uniform')(X_in)\n",
    "        X = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "        X = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "        X = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(X)\n",
    "        model = Model(inputs = X_in, outputs = X, name='CartPole DQN model')\n",
    "        model.compile(loss=\"mse\", optimizer=RMSprop(lr=self.learning_rate, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        if random.uniform(0,1) <= self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state)[0])\n",
    "            \n",
    "    def replay(self):\n",
    "        if len(self.memory) < 850:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n",
    "\n",
    "        state = np.zeros((self.batch_size, self.state_size))\n",
    "        next_state = np.zeros((self.batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            state[i] = minibatch[i][0]\n",
    "            action.append(minibatch[i][1])\n",
    "            reward.append(minibatch[i][2])\n",
    "            next_state[i] = minibatch[i][3]\n",
    "            done.append(minibatch[i][4])\n",
    "\n",
    "        target = self.model.predict(state)\n",
    "        target_next = self.model.predict(next_state)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] = reward[i] + self.gamma * (np.amax(target_next[i]))\n",
    "        self.model.fit(state, target, batch_size=self.batch_size, verbose=0)\n",
    "    \n",
    "    def exploration(self):\n",
    "        if len(self.memory) > 850:\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def train(self):\n",
    "        episodes=[]\n",
    "        final_scores=[]\n",
    "        for e in range(500): # maximum episodes to train = 500\n",
    "            \n",
    "            state = self.env.reset() #reseting env for each episode\n",
    "            state = np.reshape(state, [1,4])\n",
    "            \n",
    "            score = 0 #initialising score for each episode\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action = self.act(state) #choosing action according to the state\n",
    "                next_state, reward, done, _ = self.env.step(action) #taking action from above step\n",
    "                next_state = np.reshape(next_state, [1,4]) \n",
    "                self.remember(state, action, reward, next_state, done) #pushing info in deque\n",
    "                state = next_state #updating state\n",
    "                self.exploration() #updating epsilon\n",
    "                score+=1 #incrementing score as the reward for survival is 1\n",
    "                \n",
    "                if done: #when game(episode) is over\n",
    "                    final_scores.append(score)\n",
    "                    episodes.append(e+1)\n",
    "                    print(\"Episode: {}, Score: {}\".format(e+1, score))\n",
    "                    if score == 500: # Stoping the training when score reaches 500 for the first time\n",
    "                        print(\"Number of episodes to train : {}\".format(e+1))\n",
    "                        print(\"--Saving trained model--\")\n",
    "                        self.model.save(\"model_t3.h5\") #saving model\n",
    "                        #Training logs\n",
    "                        plt.figure(1) \n",
    "                        plt.plot(episodes,final_scores)\n",
    "                        plt.xlabel('Episodes')\n",
    "                        plt.ylabel('Scores')\n",
    "                        plt.title('Training')\n",
    "                        plt.show()\n",
    "                        return\n",
    "                self.replay() #updating weights of NN for better prediction                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent = Agent()\n",
    "agent.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
